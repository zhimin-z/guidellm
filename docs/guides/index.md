---
weight: -5
---

# Guides

Welcome to the GuideLLM guides section! Here you'll find comprehensive documentation covering key components and concepts of the GuideLLM platform. These guides will help you understand the inner workings of GuideLLM, how to configure its various components, and how to interpret benchmark results to optimize your LLM deployments.

Whether you're interested in understanding the system architecture, exploring supported backends, configuring datasets, analyzing metrics, or setting service level objectives, these guides provide the detailed information you need to make informed decisions about your LLM deployments.

## Key Guides

<div class="grid cards" markdown>

- :material-layers-outline:{ .lg .middle } Architecture

  ______________________________________________________________________

  Understanding the modular design of GuideLLM and how core components interact to evaluate LLM deployments.

  [:octicons-arrow-right-24: Architecture Overview](architecture.md)

- :material-server:{ .lg .middle } Backends

  ______________________________________________________________________

  Learn about supported LLM backends and how to set up OpenAI-compatible servers for benchmarking.

  [:octicons-arrow-right-24: Backend Guide](backends.md)

- :material-database:{ .lg .middle } Datasets

  ______________________________________________________________________

  Configure and use different data sources for benchmarking, including synthetic data, Hugging Face datasets, and file-based options.

  [:octicons-arrow-right-24: Dataset Guide](datasets.md)

- :material-chart-bar:{ .lg .middle } Metrics

  ______________________________________________________________________

  Explore the comprehensive metrics provided by GuideLLM to evaluate performance, including latency, throughput, and token-level analysis.

  [:octicons-arrow-right-24: Metrics Guide](metrics.md)

- :material-file-export:{ .lg .middle } Outputs

  ______________________________________________________________________

  Learn about supported output formats and how to customize result reporting for your benchmarks.

  [:octicons-arrow-right-24: Output Guide](outputs.md)

- :material-target:{ .lg .middle } Service Level Objectives

  ______________________________________________________________________

  Define and implement SLOs and SLAs for your LLM deployments to ensure reliability and performance.

  [:octicons-arrow-right-24: SLO Guide](service_level_objectives.md)

- :material-stop-circle-outline:{ .lg .middle } Over-Saturation Stopping

  ______________________________________________________________________

  Automatically detect and stop benchmarks when models become over-saturated to prevent wasted compute resources and ensure valid results.

  [:octicons-arrow-right-24: Over-Saturation Guide](over_saturation_stopping.md)

- :material-check-circle:{ .lg .middle } Unified Evaluation Workflow

  ______________________________________________________________________

  Review which strategies from the unified evaluation workflow across 50+ frameworks are supported natively by GuideLLM.

  [:octicons-arrow-right-24: Workflow Support](unified_evaluation_workflow.md)

</div>
